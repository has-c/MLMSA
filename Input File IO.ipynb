{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit965da26d4e584d47b9c0c057f21f2712",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xy_file(file_url):\n",
    "    #Open the file with the spectra and read the contents\n",
    "    input_df = pd.read_table(file_url, delimiter=\" \", header=None, names=['m/z', 'intensity'])\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_url = 'Data//XY Data//'\n",
    "output_data_url = 'Data//CSV Data//'\n",
    "input_files = sorted(listdir(input_data_url))\n",
    "input_files = [file for file in input_files if file != '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c_1.xy\nc_10.xy\nc_11.xy\nc_12.xy\nc_13.xy\nc_14_1.xy\nc_14_2.xy\nc_15_1.xy\nc_15_2.xy\nc_16_1.xy\nc_16_2.xy\nc_16_3.xy\nc_17.xy\nc_18_1.xy\nc_18_2.xy\nc_19_1.xy\nc_19_2.xy\nc_19_3.xy\nc_2.xy\nc_3_1.xy\nc_3_2.xy\nc_4.xy\nc_5_1.xy\nc_5_2.xy\nc_5_3.xy\nc_6_1.xy\nc_6_2.xy\nc_6_3.xy\nc_7.xy\nc_8_1.xy\nc_8_2.xy\nc_9.xy\ndemo_4.xy\no_1_1.xy\no_1_2.xy\no_1_3.xy\no_2.xy\no_3_1.xy\no_3_2.xy\no_4.xy\no_5_1.xy\no_5_2.xy\no_5_3.xy\no_6.xy\nt_1_1.xy\nt_1_2.xy\nt_1_3.xy\nt_1_4.xy\nt_2.xy\nt_3.xy\nt_4_1.xy\nt_4_2.xy\nt_5_1.xy\nt_5_2.xy\nub_1_1.xy\nub_1_2.xy\nub_2.xy\nub_3.xy\nub_4_1.xy\nub_4_2.xy\nub_5_1.xy\nub_5_2.xy\n"
    }
   ],
   "source": [
    "#concatenate different spectrums \n",
    "processed_files = []\n",
    "for file in input_files: \n",
    "    print(file)\n",
    "\n",
    "    #check whether spectrum has been processed\n",
    "    if file not in processed_files: \n",
    "\n",
    "        #assess if the spectrum is split into multiple files\n",
    "        #if name contains more than 1 underscore then yes it is split\n",
    "        file_name_split = file.split(\"_\")\n",
    "        split_spectrum = len(file_name_split) > 2\n",
    "        \n",
    "\n",
    "        if split_spectrum:\n",
    "            #find the other files with the same name and process them \n",
    "            pattern = re.compile(\"^\"+file_name_split[0]+\"_\"+file_name_split[1])\n",
    "            matches = [file for file in input_files if pattern.match(file)]\n",
    "            \n",
    "            #load matched spectrums, concatenate together and write out\n",
    "            input_df = pd.DataFrame([])\n",
    "            for file in matches:\n",
    "                if input_df.size == 0:\n",
    "                    input_df = read_xy_file(input_data_url+file)\n",
    "                else: \n",
    "                    df = read_xy_file(input_data_url+file)\n",
    "                    input_df = pd.concat([input_df, df], ignore_index=True)\n",
    "                    \n",
    "                #file has been processed\n",
    "                processed_files.append(file)\n",
    "            \n",
    "            #write out entire spectrum\n",
    "            output_filename = file_name_split[0] + \"_\" + file_name_split[1] + \".csv\"\n",
    "            input_df.to_csv(output_data_url+ output_filename, index=False)\n",
    "\n",
    "        else:\n",
    "            output_filename = file_name_split[0] + \"_\" + file_name_split[1].split(\".\")[0] + \".csv\"\n",
    "\n",
    "            #spectrum is not split so just save it as a csv\n",
    "            input_df = read_xy_file(input_data_url+file)\n",
    "            input_df.to_csv(output_data_url+ output_filename, index=False)\n",
    "\n",
    "            #add file to processed files\n",
    "            processed_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}